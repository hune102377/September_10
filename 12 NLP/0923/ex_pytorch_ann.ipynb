{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn                            # 신경망모듈\n",
    "from torch.utils.data import DataLoader         # 데이터 로딩 관련 모듈\n",
    "from torchvision import datasets, transforms    # torch저장소 데이터 셋 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1] 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Tensor의 저장소 설정을 위한 체크\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] 신경망 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순차적인 Layer 구성 => Sequential, Layer...\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    # 생성자 메서드, NeuralNetwork() 객체 생성 시 실행\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # 입력층\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 은닉층\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 출력층\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        \n",
    "    # NN 모델이 동작을 시작할 떄 실행\n",
    "    def forward(self, x):\n",
    "        # 1차원으로 데이터 변환\n",
    "        x = self.flatten(x)\n",
    "        # NN 모델에 입력 데이터 전달\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN 모델 생성\n",
    "mnistM = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mnistM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "X = torch.rand(1, 28, 28, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "inptT=torch.rand(1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3.7725e-01, 1.5910e-01, 9.6034e-02, 8.6071e-01, 1.0118e-01,\n",
      "          1.8592e-01, 5.2255e-01, 3.9052e-01, 7.7449e-01, 2.0657e-01,\n",
      "          2.2475e-01, 4.0996e-01, 1.2159e-01, 3.9696e-01, 6.3821e-01,\n",
      "          2.9391e-01, 6.3074e-01, 9.9941e-01, 5.0215e-01, 7.4630e-01,\n",
      "          1.6576e-01, 5.1425e-02, 7.4109e-01, 6.1974e-01, 1.7814e-01,\n",
      "          6.1162e-01, 6.6012e-02, 6.9078e-01],\n",
      "         [9.3643e-01, 5.1638e-01, 4.0024e-01, 3.8758e-01, 1.7174e-01,\n",
      "          1.6643e-01, 7.7668e-02, 6.1221e-02, 4.6311e-01, 6.3531e-01,\n",
      "          8.3729e-01, 6.9308e-01, 4.1471e-01, 2.0229e-01, 3.2776e-01,\n",
      "          7.1187e-01, 3.2500e-01, 2.3862e-01, 3.3156e-01, 2.0628e-01,\n",
      "          1.6535e-01, 9.5190e-01, 5.6204e-01, 9.0037e-01, 2.2389e-01,\n",
      "          6.9918e-01, 4.4861e-01, 7.5846e-01],\n",
      "         [2.9882e-02, 2.1264e-01, 7.5797e-01, 8.4103e-02, 9.9271e-01,\n",
      "          9.0550e-01, 6.6602e-01, 6.0551e-01, 5.7866e-01, 3.7327e-01,\n",
      "          4.8497e-01, 3.4007e-01, 5.2773e-01, 3.9373e-01, 9.7249e-01,\n",
      "          9.2214e-01, 7.5849e-01, 2.6423e-01, 9.8226e-01, 9.8150e-01,\n",
      "          7.8922e-01, 2.1302e-01, 9.1407e-02, 9.7920e-02, 5.3765e-02,\n",
      "          6.5072e-01, 5.1085e-01, 1.2501e-01],\n",
      "         [2.2823e-01, 9.2384e-01, 4.5147e-01, 2.2991e-01, 2.5498e-01,\n",
      "          7.0638e-01, 6.5597e-01, 7.1718e-01, 4.7394e-01, 2.4702e-01,\n",
      "          5.8040e-01, 8.3909e-01, 4.2070e-01, 5.2495e-01, 4.3407e-01,\n",
      "          4.1141e-01, 5.3257e-01, 5.9977e-01, 7.6485e-01, 3.5800e-02,\n",
      "          1.7511e-01, 3.5830e-01, 5.5153e-02, 6.3401e-01, 9.8813e-02,\n",
      "          6.1622e-02, 8.1632e-01, 7.8592e-01],\n",
      "         [6.6084e-01, 1.8054e-01, 4.2442e-02, 9.8709e-01, 1.5506e-01,\n",
      "          8.2021e-01, 4.1503e-01, 9.7592e-01, 9.9671e-01, 6.5839e-02,\n",
      "          1.2564e-01, 6.9454e-01, 2.3614e-02, 7.5327e-01, 2.1746e-01,\n",
      "          4.8962e-01, 2.7234e-01, 7.4850e-01, 5.3579e-01, 2.8498e-02,\n",
      "          9.7703e-01, 1.2606e-01, 2.0295e-01, 9.5460e-01, 6.1560e-01,\n",
      "          9.2076e-01, 4.6556e-02, 3.0575e-01],\n",
      "         [1.8635e-01, 2.6042e-03, 5.9602e-01, 7.0000e-01, 7.6901e-01,\n",
      "          6.8344e-01, 2.9158e-01, 5.5691e-01, 9.8090e-01, 1.3735e-01,\n",
      "          8.3167e-01, 8.0605e-02, 2.0475e-01, 4.9266e-01, 8.6043e-02,\n",
      "          7.7343e-01, 7.5048e-01, 6.9014e-01, 2.8212e-01, 7.5989e-01,\n",
      "          5.6253e-02, 7.3555e-01, 9.3354e-01, 2.1840e-01, 1.0269e-01,\n",
      "          5.1272e-01, 6.8353e-02, 6.2617e-01],\n",
      "         [6.8070e-01, 6.3400e-02, 2.7365e-02, 2.2444e-01, 8.7138e-01,\n",
      "          9.5220e-01, 3.4808e-01, 2.2213e-01, 6.9831e-01, 4.5325e-01,\n",
      "          7.4020e-01, 6.9524e-01, 3.7641e-01, 1.8113e-01, 9.7856e-01,\n",
      "          7.8723e-01, 3.8085e-01, 3.5165e-01, 4.2220e-01, 2.1116e-01,\n",
      "          2.7243e-01, 1.9001e-01, 8.1402e-01, 8.1130e-01, 7.9909e-01,\n",
      "          4.3375e-02, 2.5007e-01, 7.3593e-01],\n",
      "         [7.3763e-01, 8.1914e-01, 7.8580e-01, 8.8150e-01, 7.3770e-01,\n",
      "          8.8745e-01, 7.8439e-01, 6.2452e-01, 4.4237e-02, 2.2497e-01,\n",
      "          1.5130e-01, 4.6285e-01, 7.9538e-01, 2.5316e-01, 1.9868e-01,\n",
      "          3.7330e-01, 8.1494e-01, 9.4677e-01, 7.7424e-01, 9.7849e-01,\n",
      "          4.7994e-01, 8.9704e-01, 6.6539e-01, 4.8447e-01, 6.2339e-03,\n",
      "          8.1859e-01, 8.2568e-02, 8.3497e-01],\n",
      "         [4.2238e-01, 7.4332e-01, 4.4745e-02, 8.2867e-01, 1.6114e-01,\n",
      "          8.0092e-01, 3.9938e-01, 8.7082e-01, 2.9339e-01, 4.4555e-02,\n",
      "          6.3503e-01, 4.4914e-01, 2.7674e-01, 5.1605e-01, 8.6853e-01,\n",
      "          9.0396e-02, 3.0142e-01, 4.4423e-01, 7.4166e-01, 5.8427e-01,\n",
      "          7.6141e-01, 9.4045e-01, 1.6599e-01, 8.4285e-01, 2.5813e-01,\n",
      "          2.5798e-01, 8.3204e-01, 4.6731e-01],\n",
      "         [6.0836e-02, 6.2041e-01, 5.4991e-01, 3.1645e-01, 3.6172e-01,\n",
      "          2.6645e-01, 1.9358e-02, 6.2342e-01, 1.6253e-01, 8.5129e-02,\n",
      "          4.7088e-01, 1.4442e-01, 3.4334e-01, 9.6946e-01, 7.3113e-01,\n",
      "          7.9187e-01, 1.4998e-01, 1.0894e-01, 1.6798e-01, 3.4080e-01,\n",
      "          6.0256e-02, 5.4455e-01, 1.6202e-01, 8.9504e-01, 2.9260e-02,\n",
      "          9.8058e-01, 5.2766e-01, 3.0742e-01],\n",
      "         [4.2084e-01, 2.5944e-01, 8.0342e-02, 7.6610e-01, 2.7559e-01,\n",
      "          7.7972e-01, 9.8215e-01, 1.6717e-01, 6.7515e-01, 8.8477e-01,\n",
      "          6.3951e-01, 7.3011e-03, 1.9443e-02, 5.5761e-01, 5.4181e-01,\n",
      "          2.2916e-01, 5.4292e-01, 1.8596e-01, 2.9277e-01, 5.7100e-01,\n",
      "          1.2490e-01, 3.4268e-01, 4.8265e-01, 2.1553e-01, 3.0693e-01,\n",
      "          6.7336e-01, 1.3179e-01, 2.1962e-03],\n",
      "         [2.7086e-01, 2.2372e-01, 3.1314e-01, 8.6682e-01, 3.2476e-01,\n",
      "          8.2175e-01, 5.8677e-02, 3.1061e-01, 5.7204e-01, 1.9964e-01,\n",
      "          1.3382e-01, 1.5178e-01, 5.8228e-01, 9.0492e-01, 5.6071e-01,\n",
      "          5.0399e-01, 2.2119e-01, 1.4261e-01, 5.6697e-02, 4.9931e-01,\n",
      "          3.0940e-01, 7.8277e-01, 1.3022e-01, 9.5165e-01, 4.8267e-01,\n",
      "          7.1833e-01, 8.9852e-01, 1.4491e-01],\n",
      "         [7.5495e-01, 3.1946e-01, 8.7333e-01, 7.8796e-02, 2.4616e-01,\n",
      "          9.9780e-01, 2.3758e-01, 8.0575e-01, 3.6334e-01, 7.7580e-01,\n",
      "          1.9259e-01, 4.1566e-01, 7.6291e-01, 6.0676e-01, 7.1925e-01,\n",
      "          8.8474e-01, 8.4793e-01, 7.9295e-01, 7.7767e-01, 1.4216e-01,\n",
      "          8.4984e-02, 4.3026e-01, 6.1351e-01, 3.6122e-01, 3.2914e-01,\n",
      "          4.2296e-01, 1.3825e-01, 5.5962e-01],\n",
      "         [6.7268e-01, 3.4162e-01, 2.8774e-01, 1.4132e-01, 8.6142e-01,\n",
      "          8.1767e-01, 5.1659e-01, 1.3628e-01, 4.3294e-01, 3.9871e-01,\n",
      "          8.9173e-01, 7.1370e-01, 3.7230e-01, 5.8232e-01, 3.5980e-01,\n",
      "          6.5170e-01, 3.3900e-01, 3.2347e-02, 9.1869e-01, 2.5048e-02,\n",
      "          9.4355e-01, 2.9791e-02, 9.5652e-01, 2.5343e-01, 7.8149e-01,\n",
      "          9.9573e-01, 5.5923e-01, 1.5930e-01],\n",
      "         [4.8444e-01, 1.6697e-01, 2.5176e-01, 7.6887e-01, 8.8525e-01,\n",
      "          1.4918e-01, 7.8245e-01, 7.6372e-01, 9.6943e-01, 4.3463e-01,\n",
      "          4.3215e-01, 2.0395e-01, 2.1333e-01, 3.0938e-01, 3.1866e-01,\n",
      "          3.2820e-01, 9.4761e-01, 1.1642e-01, 5.3846e-02, 2.0115e-01,\n",
      "          3.8093e-01, 7.4369e-01, 4.9516e-01, 9.8577e-01, 2.0421e-01,\n",
      "          3.5587e-01, 6.9051e-01, 9.6135e-01],\n",
      "         [3.9005e-01, 5.1888e-01, 8.9370e-02, 7.6736e-01, 9.0365e-01,\n",
      "          1.5970e-01, 6.7329e-04, 3.8138e-02, 9.7756e-01, 2.7585e-01,\n",
      "          9.9151e-01, 6.1095e-01, 7.8725e-01, 3.8139e-01, 6.9685e-01,\n",
      "          5.5726e-01, 6.7640e-01, 4.0544e-01, 6.4625e-01, 8.1716e-01,\n",
      "          2.4989e-01, 9.8053e-01, 2.1276e-02, 7.1990e-01, 9.2978e-01,\n",
      "          3.9373e-01, 5.7940e-01, 9.1782e-01],\n",
      "         [9.6726e-01, 4.7170e-01, 7.4736e-01, 2.4868e-01, 5.2519e-01,\n",
      "          7.3427e-01, 6.8169e-01, 4.4094e-01, 7.3343e-01, 8.7566e-01,\n",
      "          5.8780e-02, 2.4719e-01, 3.5627e-01, 5.4385e-01, 3.4426e-01,\n",
      "          9.0453e-01, 2.5063e-01, 4.9573e-01, 3.0112e-02, 2.9027e-01,\n",
      "          9.9328e-01, 9.1488e-01, 2.2404e-01, 2.5655e-01, 6.5620e-01,\n",
      "          7.0511e-01, 8.6650e-01, 8.7610e-01],\n",
      "         [9.0935e-01, 8.8825e-01, 9.4823e-01, 1.1177e-01, 3.5047e-01,\n",
      "          6.9713e-01, 8.2841e-01, 1.6533e-01, 4.6923e-01, 4.5569e-02,\n",
      "          7.4023e-01, 3.3021e-01, 1.1324e-01, 3.3780e-01, 5.3676e-01,\n",
      "          3.6870e-02, 3.1063e-01, 9.2163e-01, 2.7346e-01, 2.5095e-01,\n",
      "          5.9741e-01, 1.2115e-01, 5.6231e-01, 3.2676e-03, 8.5435e-02,\n",
      "          3.8242e-02, 7.9697e-01, 3.3849e-02],\n",
      "         [7.9066e-01, 1.3547e-02, 4.6507e-01, 6.7609e-01, 7.0489e-01,\n",
      "          1.2919e-01, 7.8220e-01, 3.8682e-01, 6.4052e-01, 6.2034e-01,\n",
      "          2.4690e-01, 5.1917e-01, 6.4341e-01, 5.8892e-01, 3.6426e-01,\n",
      "          3.8386e-02, 5.3822e-01, 8.9139e-01, 5.2297e-01, 4.3801e-01,\n",
      "          9.1714e-01, 7.4185e-02, 2.5780e-02, 3.2805e-01, 7.1269e-01,\n",
      "          1.9729e-01, 6.4747e-01, 8.4281e-01],\n",
      "         [8.8435e-01, 1.4052e-01, 6.3969e-01, 7.7211e-03, 3.6648e-01,\n",
      "          8.7424e-01, 3.3905e-01, 7.1801e-01, 8.2795e-03, 6.8343e-01,\n",
      "          8.4584e-01, 4.3943e-01, 2.8803e-01, 6.7128e-01, 2.3258e-01,\n",
      "          1.7703e-01, 6.4766e-01, 4.7564e-01, 5.8514e-01, 4.3545e-01,\n",
      "          4.7994e-01, 8.4738e-01, 5.1995e-01, 1.3992e-01, 2.3066e-01,\n",
      "          9.4199e-01, 8.2161e-01, 4.1889e-01],\n",
      "         [4.0090e-01, 2.4137e-01, 7.8859e-01, 1.3219e-01, 2.6460e-01,\n",
      "          1.6708e-02, 3.7403e-01, 1.2408e-01, 9.8959e-02, 2.1887e-01,\n",
      "          2.6145e-01, 5.9924e-01, 6.4487e-01, 8.7763e-01, 7.4794e-01,\n",
      "          7.2266e-01, 9.7633e-01, 2.3940e-01, 7.5858e-01, 9.4914e-01,\n",
      "          6.3198e-01, 4.9321e-01, 2.7097e-01, 4.1916e-01, 8.9981e-01,\n",
      "          6.2513e-02, 6.6252e-02, 5.1819e-01],\n",
      "         [7.9165e-01, 9.9970e-01, 9.4644e-01, 6.5424e-01, 6.4362e-01,\n",
      "          3.5152e-01, 2.1685e-01, 9.8786e-01, 9.8908e-01, 8.2497e-01,\n",
      "          7.5776e-01, 4.4946e-02, 7.0163e-01, 1.5163e-01, 8.7640e-01,\n",
      "          8.4467e-01, 5.1511e-01, 1.0342e-01, 4.3326e-01, 2.0680e-01,\n",
      "          1.6680e-01, 6.6356e-02, 2.3479e-01, 2.6711e-01, 1.8475e-01,\n",
      "          6.7907e-01, 1.2425e-01, 2.9045e-01],\n",
      "         [5.2149e-03, 1.8799e-01, 7.0956e-01, 2.7467e-03, 6.8952e-01,\n",
      "          7.7631e-01, 5.3752e-01, 5.0514e-01, 5.3084e-01, 2.0072e-01,\n",
      "          8.2656e-01, 3.0315e-01, 8.3519e-01, 6.3452e-01, 8.8345e-01,\n",
      "          9.7224e-01, 3.3778e-01, 7.1557e-01, 9.2183e-01, 9.5356e-01,\n",
      "          5.4135e-01, 4.0681e-01, 5.1993e-01, 6.1269e-01, 1.3646e-02,\n",
      "          5.5596e-01, 7.5474e-01, 9.3774e-01],\n",
      "         [4.4609e-01, 3.9800e-01, 5.7855e-01, 1.8785e-01, 9.2320e-01,\n",
      "          1.5584e-01, 7.4410e-01, 1.8505e-01, 9.1138e-01, 4.9217e-01,\n",
      "          9.4442e-01, 7.1449e-01, 1.1372e-01, 4.4523e-01, 7.3575e-02,\n",
      "          1.6129e-01, 4.2078e-01, 1.9220e-01, 1.6424e-01, 7.8265e-01,\n",
      "          5.8982e-01, 6.9158e-01, 6.3255e-01, 6.5985e-01, 9.8584e-01,\n",
      "          1.2789e-01, 6.5207e-01, 6.4990e-01],\n",
      "         [4.9144e-01, 8.7213e-02, 3.0469e-02, 2.6125e-01, 7.2671e-01,\n",
      "          6.8377e-01, 6.1792e-01, 2.4951e-01, 5.3106e-01, 9.4125e-02,\n",
      "          5.0029e-01, 7.3687e-01, 1.5277e-01, 4.2180e-01, 9.5886e-01,\n",
      "          3.4956e-01, 8.3057e-01, 9.2341e-01, 9.4853e-01, 6.8184e-01,\n",
      "          8.5499e-01, 3.0460e-01, 4.2334e-01, 7.0325e-01, 2.4945e-01,\n",
      "          1.7713e-02, 3.5353e-01, 3.9449e-01],\n",
      "         [4.5609e-01, 5.4995e-01, 7.6288e-01, 1.5644e-01, 9.3248e-02,\n",
      "          1.7378e-02, 1.2824e-01, 6.7689e-01, 2.5929e-01, 5.8780e-01,\n",
      "          7.3910e-01, 1.0860e-02, 4.9416e-01, 1.4429e-01, 9.6366e-01,\n",
      "          4.3699e-01, 7.8239e-02, 6.3430e-02, 9.0338e-01, 8.3259e-01,\n",
      "          7.9833e-01, 1.9249e-01, 7.2137e-01, 4.3930e-01, 9.6963e-01,\n",
      "          8.6019e-01, 8.8989e-01, 3.5023e-02],\n",
      "         [9.8950e-01, 6.7843e-01, 4.3002e-01, 7.9482e-01, 3.6845e-01,\n",
      "          1.0966e-02, 3.7688e-01, 3.6101e-01, 3.3528e-01, 5.8874e-01,\n",
      "          8.0715e-01, 4.9100e-02, 4.0106e-01, 1.5778e-01, 6.8157e-03,\n",
      "          5.3540e-02, 4.2149e-01, 2.0017e-02, 4.2331e-01, 3.9588e-01,\n",
      "          9.1616e-01, 6.2575e-01, 2.0205e-02, 1.0925e-01, 7.2247e-01,\n",
      "          1.0760e-01, 8.0812e-01, 3.1295e-02],\n",
      "         [3.8323e-01, 7.6098e-01, 7.2271e-01, 3.8193e-01, 3.8441e-01,\n",
      "          2.4025e-01, 6.5068e-01, 7.7481e-01, 5.4097e-01, 6.2825e-01,\n",
      "          8.9784e-01, 1.4174e-01, 6.2743e-01, 9.9918e-01, 4.1676e-01,\n",
      "          7.2566e-01, 6.6025e-01, 7.4162e-01, 2.3302e-01, 6.2189e-02,\n",
      "          6.0981e-01, 3.1337e-01, 8.3794e-01, 5.1157e-01, 2.0703e-01,\n",
      "          9.3012e-02, 6.8247e-02, 2.5429e-01]]])\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "logits = mnistM(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1540,  0.0214, -0.1131, -0.0826, -0.0724, -0.0207,  0.0474,  0.0510,\n",
       "         -0.0006, -0.0607]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class : tensor([7])\n"
     ]
    }
   ],
   "source": [
    "# 결과 분석\n",
    "pred_probab = nn.Softmax(dim = 1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f'Predicted class : {y_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "399b942a90945adf822711c817a2148b24a3a269b52160aecaa333816877df5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
